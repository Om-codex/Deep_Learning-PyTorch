{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48fc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e696674c",
   "metadata": {},
   "source": [
    "## **Building a 2-Layered Neural Network From Scratch (NumPy Version)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "52badd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xavier_init for tanh,sigmoid\n",
    "def xavier_init(fan_in,fan_out):\n",
    "  return np.random.randn(fan_in,fan_out) * np.sqrt(1.0/fan_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "600a4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# he_init for relu\n",
    "def he_init(fan_in,fan_out):\n",
    "  return np.random.randn(fan_in,fan_out) * np.sqrt(2.0/fan_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab7153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Initialization\n",
    "def init_params(input_dim,hidden_dim,output_dim):\n",
    "  W1 = np.random.randn(input_dim,hidden_dim)*0.01 # here 2x2 matrix of random values of weight and multiplied by 0.01 to make it small value\n",
    "  b1 = np.zeros((1,hidden_dim)) # bias can be zero\n",
    "  W2 = np.random.randn(hidden_dim,output_dim)*0.01 # here also 4x4 matrix of hidden layer by output layer and made small\n",
    "  b2 = np.zeros((1,output_dim)) # bias can be zero\n",
    "  return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8b16f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# he Weight Initialization\n",
    "def he_init_params(input_dim,hidden_dim,output_dim):\n",
    "  W1 = he_init(input_dim,hidden_dim) # here 2x2 matrix of random values of weight and multiplied by 0.01 to make it small value\n",
    "  b1 = np.zeros((1,hidden_dim)) # bias can be zero\n",
    "  W2 = he_init(hidden_dim,output_dim) # here also 4x4 matrix of hidden layer by output layer and made small\n",
    "  b2 = np.zeros((1,output_dim)) # bias can be zero\n",
    "  return W1,b1,W2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "def relu(x):\n",
    "  return np.maximum(0,x)  # returns x if greater than 0\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float) # converts the bool into float like if x > 0 => True then float(True) is 1 same like derivative of relu\n",
    "\n",
    "def tanh(x):\n",
    "   return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def tanh_deriv(x):\n",
    "   a = np.tanh(x)\n",
    "   return (1 - a ** 2)\n",
    "\n",
    "def sigmoid(x):\n",
    "   return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(s):\n",
    "   return s * (1 - s)\n",
    "\n",
    "def softmax(z):\n",
    "  exp = np.exp(z - np.max(z, axis = 1, keepdims = True))\n",
    "  return exp/np.sum(exp, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d7cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "def forward(x, w1, b1, w2, b2):\n",
    "  z1 = x @ w1 + b1 # calculation of node 1\n",
    "  a1 = relu(z1) # output of node 1\n",
    "  z2 = a1 @ w2 + b2 # calculation of node 2\n",
    "  a2 = softmax(z2) # output of node 2 predictions (in probabilites)\n",
    "  cache = (x,z1,a1,z2,a2)\n",
    "  return a2,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8966f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy loss function\n",
    "def cross_entropy(y_pred, y_true):\n",
    "  m = y_true.shape[0] # m means the number of samples in the y_true\n",
    "  return - np.sum(y_true * np.log(y_pred + 1e-9))/ m # here y_true is multiplied with the predicted values so that only the true predicted values survive \n",
    "\n",
    "# for example if true label is class 0 [1,0,0] and y_pred is [0.7,0.2,0.1] where y_pred is confident about the class 0 so after y_true * np.log(y_pred) gives [-0.51,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cebba98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "\n",
    "def backward(cache,w2,y_true):\n",
    " x,z1,a1,z2,a2 = cache\n",
    " m = x.shape[0]\n",
    "\n",
    " dz2 = a2 - y_true # calculate the loss of the output neuron\n",
    "\n",
    " dw2 = (a1.T @ dz2)/m # calculate the contribution and error by w2 for the output z2 with the output of hidden layer a1\n",
    " db2 = np.sum(dz2,axis = 0,keepdims = True)/m # the bias b2 contributes to z2 directly so we just sum up all the erros of z2 as dz2 and divide by the number of samples\n",
    "\n",
    " da1 = dz2 @ w2.T  # How much did each hidden neuron (A1) contribute to the output error (dZ2)? where W2 tells us how strongly each hidden neuron influences the output.\n",
    " dz1 = da1 * relu_deriv(z1) # how much is the hidden layers errors flowing through the neurons as da1 along with the applied relu as relu_deriv\n",
    "\n",
    " dw1 = (x.T @ dz1) / m # calculate the contribution and error by w1 for the output z1 with the input x\n",
    " db1 = np.sum(dz1, axis=0, keepdims=True) / m # the bias b1 contributes to z1 directly so we just sum up all the errors of z1 as dz1 and divide by the number of samples\n",
    "\n",
    "\n",
    " return dw1, db1, dw2, db2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a2a3f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_forward(a,p):\n",
    "  mask = (np.random.randn(*a.shape) < p)\n",
    "  out = a * mask/p\n",
    "  return out,mask\n",
    "\n",
    "def dropout_backward(dout,mask,p):\n",
    "  da = dout * mask / p\n",
    "  return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd4437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update rule\n",
    "\n",
    "def update(w1,b1,w2,b2,grads,lr):\n",
    "  dw1,db1,dw2,db2 = grads\n",
    "  w1 -= lr * dw1\n",
    "  b1 -= lr * db1\n",
    "  w2 -= lr * dw2\n",
    "  b2 -= lr * db2\n",
    "\n",
    "  return w1,b1,w2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "feb205b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_update(W, dW, lr, lam):\n",
    "    # lam = Î» (L2 coefficient)\n",
    "    W -= lr * (dW + lam * W)\n",
    "    return W\n",
    "\n",
    "def l1_update(W, dW, lr, lam):\n",
    "    W -= lr * (dW + lam * np.sign(W))\n",
    "    return W\n",
    "\n",
    "def momentum_update(W, dW, v, lr, beta):\n",
    "    # v: velocity, same shape as W\n",
    "    v[:] = beta * v + (1.0 - beta) * dW\n",
    "    W -= lr * v\n",
    "    return W, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f6de269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_update(W, dW, m, v, t, lr,\n",
    "                beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    # m, v: first and second moment, same shape as W\n",
    "    # t: time step (int) starting from 1\n",
    "\n",
    "    m[:] = beta1 * m + (1.0 - beta1) * dW\n",
    "    v[:] = beta2 * v + (1.0 - beta2) * (dW ** 2)\n",
    "\n",
    "    m_hat = m / (1.0 - beta1 ** t)\n",
    "    v_hat = v / (1.0 - beta2 ** t)\n",
    "\n",
    "    W -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "    return W, m, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ba8529c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "def train(X,y,input_dim,hidden_dim,output_dim,lr = 0.1, epochs = 100):\n",
    "  w1 , b1, w2, b2 = init_params(input_dim,hidden_dim,output_dim) # initialize parameters\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "   y_pred, cache = forward(X, w1, b1, w2, b2) # Forward prop\n",
    "   loss = cross_entropy(y_pred,y) # calculate loss\n",
    "   grads = backward(cache,w2,y)\n",
    "   w1,b1,w2,b2 = update(w1,b1,w2,b2,grads,lr)\n",
    "   if epoch % 10 == 0:\n",
    "     print(f'Epoch {epoch+10} Loss:{loss:.4f}')\n",
    "\n",
    "  return w1,b1,w2,b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e65a4",
   "metadata": {},
   "source": [
    "## XOR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "14705845",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xor = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "dd665927",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e21ef45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "06115ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_one_hot(y, num_classes=2):\n",
    "    m = y.shape[0]\n",
    "    oh = np.zeros((m, num_classes))\n",
    "    oh[np.arange(m), y] = 1\n",
    "    return oh\n",
    "\n",
    "y_xor = to_one_hot(y_labels, num_classes=2)  # shape: (4, 2)\n",
    "y_xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e4ddde93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss:0.6931\n",
      "Epoch 20 Loss:0.6930\n",
      "Epoch 30 Loss:0.6927\n",
      "Epoch 40 Loss:0.6918\n",
      "Epoch 50 Loss:0.6889\n",
      "Epoch 60 Loss:0.6799\n",
      "Epoch 70 Loss:0.6552\n",
      "Epoch 80 Loss:0.6050\n",
      "Epoch 90 Loss:0.5502\n",
      "Epoch 100 Loss:0.5155\n",
      "Epoch 110 Loss:0.4986\n",
      "Epoch 120 Loss:0.4900\n",
      "Epoch 130 Loss:0.4760\n",
      "Epoch 140 Loss:0.4423\n",
      "Epoch 150 Loss:0.3425\n",
      "Epoch 160 Loss:0.2137\n",
      "Epoch 170 Loss:0.1219\n",
      "Epoch 180 Loss:0.0860\n",
      "Epoch 190 Loss:0.0614\n",
      "Epoch 200 Loss:0.0501\n",
      "Epoch 210 Loss:0.0403\n",
      "Epoch 220 Loss:0.0344\n",
      "Epoch 230 Loss:0.0293\n",
      "Epoch 240 Loss:0.0261\n",
      "Epoch 250 Loss:0.0231\n",
      "Epoch 260 Loss:0.0207\n",
      "Epoch 270 Loss:0.0186\n",
      "Epoch 280 Loss:0.0170\n",
      "Epoch 290 Loss:0.0157\n",
      "Epoch 300 Loss:0.0145\n",
      "Epoch 310 Loss:0.0135\n",
      "Epoch 320 Loss:0.0126\n",
      "Epoch 330 Loss:0.0119\n",
      "Epoch 340 Loss:0.0111\n",
      "Epoch 350 Loss:0.0105\n",
      "Epoch 360 Loss:0.0099\n",
      "Epoch 370 Loss:0.0094\n",
      "Epoch 380 Loss:0.0089\n",
      "Epoch 390 Loss:0.0086\n",
      "Epoch 400 Loss:0.0082\n",
      "Epoch 410 Loss:0.0078\n",
      "Epoch 420 Loss:0.0075\n",
      "Epoch 430 Loss:0.0071\n",
      "Epoch 440 Loss:0.0069\n",
      "Epoch 450 Loss:0.0066\n",
      "Epoch 460 Loss:0.0064\n",
      "Epoch 470 Loss:0.0062\n",
      "Epoch 480 Loss:0.0060\n",
      "Epoch 490 Loss:0.0058\n",
      "Epoch 500 Loss:0.0056\n",
      "Epoch 510 Loss:0.0054\n",
      "Epoch 520 Loss:0.0052\n",
      "Epoch 530 Loss:0.0051\n",
      "Epoch 540 Loss:0.0050\n",
      "Epoch 550 Loss:0.0048\n",
      "Epoch 560 Loss:0.0047\n",
      "Epoch 570 Loss:0.0046\n",
      "Epoch 580 Loss:0.0044\n",
      "Epoch 590 Loss:0.0043\n",
      "Epoch 600 Loss:0.0042\n",
      "Epoch 610 Loss:0.0041\n",
      "Epoch 620 Loss:0.0040\n",
      "Epoch 630 Loss:0.0039\n",
      "Epoch 640 Loss:0.0038\n",
      "Epoch 650 Loss:0.0038\n",
      "Epoch 660 Loss:0.0037\n",
      "Epoch 670 Loss:0.0036\n",
      "Epoch 680 Loss:0.0035\n",
      "Epoch 690 Loss:0.0034\n",
      "Epoch 700 Loss:0.0034\n",
      "Epoch 710 Loss:0.0033\n",
      "Epoch 720 Loss:0.0032\n",
      "Epoch 730 Loss:0.0032\n",
      "Epoch 740 Loss:0.0031\n",
      "Epoch 750 Loss:0.0031\n",
      "Epoch 760 Loss:0.0030\n",
      "Epoch 770 Loss:0.0030\n",
      "Epoch 780 Loss:0.0029\n",
      "Epoch 790 Loss:0.0029\n",
      "Epoch 800 Loss:0.0028\n",
      "Epoch 810 Loss:0.0028\n",
      "Epoch 820 Loss:0.0027\n",
      "Epoch 830 Loss:0.0027\n",
      "Epoch 840 Loss:0.0026\n",
      "Epoch 850 Loss:0.0026\n",
      "Epoch 860 Loss:0.0025\n",
      "Epoch 870 Loss:0.0025\n",
      "Epoch 880 Loss:0.0025\n",
      "Epoch 890 Loss:0.0024\n",
      "Epoch 900 Loss:0.0024\n",
      "Epoch 910 Loss:0.0024\n",
      "Epoch 920 Loss:0.0023\n",
      "Epoch 930 Loss:0.0023\n",
      "Epoch 940 Loss:0.0023\n",
      "Epoch 950 Loss:0.0022\n",
      "Epoch 960 Loss:0.0022\n",
      "Epoch 970 Loss:0.0022\n",
      "Epoch 980 Loss:0.0021\n",
      "Epoch 990 Loss:0.0021\n",
      "Epoch 1000 Loss:0.0021\n",
      "Probabilities:\n",
      " [[9.96696676e-01 3.30332402e-03]\n",
      " [8.17997082e-04 9.99182003e-01]\n",
      " [8.32277991e-04 9.99167722e-01]\n",
      " [9.96696676e-01 3.30332402e-03]]\n",
      "Predicted classes: [0 1 1 0]\n",
      "True classes: [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "w1,b1,w2,b2 = train(X_xor,y_xor,2,4,2,lr = 0.5,epochs = 1000)\n",
    "\n",
    "y_pred_xor, _ = forward(X_xor,w1,b1,w2,b2)\n",
    "print(\"Probabilities:\\n\", y_pred_xor)\n",
    "print(\"Predicted classes:\", np.argmax(y_pred_xor, axis=1))\n",
    "print(\"True classes:\", np.argmax(y_xor, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d511a",
   "metadata": {},
   "source": [
    "## Circle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ea0f5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X_circle,y_circle = make_circles(n_samples = 1000,factor = .5, noise = 0.03,random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6c56ff5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_circle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "303103e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_circle = to_one_hot(y_circle,num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "78df22c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss:0.6931\n",
      "Epoch 20 Loss:0.6931\n",
      "Epoch 30 Loss:0.6931\n",
      "Epoch 40 Loss:0.6930\n",
      "Epoch 50 Loss:0.6929\n",
      "Epoch 60 Loss:0.6927\n",
      "Epoch 70 Loss:0.6924\n",
      "Epoch 80 Loss:0.6919\n",
      "Epoch 90 Loss:0.6910\n",
      "Epoch 100 Loss:0.6896\n",
      "Epoch 110 Loss:0.6872\n",
      "Epoch 120 Loss:0.6835\n",
      "Epoch 130 Loss:0.6780\n",
      "Epoch 140 Loss:0.6702\n",
      "Epoch 150 Loss:0.6593\n",
      "Epoch 160 Loss:0.6444\n",
      "Epoch 170 Loss:0.6233\n",
      "Epoch 180 Loss:0.5939\n",
      "Epoch 190 Loss:0.5540\n",
      "Epoch 200 Loss:0.5032\n",
      "Epoch 210 Loss:0.4448\n",
      "Epoch 220 Loss:0.3861\n",
      "Epoch 230 Loss:0.3325\n",
      "Epoch 240 Loss:0.2867\n",
      "Epoch 250 Loss:0.2487\n",
      "Epoch 260 Loss:0.2178\n",
      "Epoch 270 Loss:0.1925\n",
      "Epoch 280 Loss:0.1718\n",
      "Epoch 290 Loss:0.1547\n",
      "Epoch 300 Loss:0.1403\n",
      "Epoch 310 Loss:0.1282\n",
      "Epoch 320 Loss:0.1179\n",
      "Epoch 330 Loss:0.1090\n",
      "Epoch 340 Loss:0.1013\n",
      "Epoch 350 Loss:0.0945\n",
      "Epoch 360 Loss:0.0886\n",
      "Epoch 370 Loss:0.0833\n",
      "Epoch 380 Loss:0.0786\n",
      "Epoch 390 Loss:0.0744\n",
      "Epoch 400 Loss:0.0706\n",
      "Epoch 410 Loss:0.0671\n",
      "Epoch 420 Loss:0.0639\n",
      "Epoch 430 Loss:0.0611\n",
      "Epoch 440 Loss:0.0584\n",
      "Epoch 450 Loss:0.0560\n",
      "Epoch 460 Loss:0.0537\n",
      "Epoch 470 Loss:0.0516\n",
      "Epoch 480 Loss:0.0497\n",
      "Epoch 490 Loss:0.0479\n",
      "Epoch 500 Loss:0.0462\n",
      "Epoch 510 Loss:0.0446\n",
      "Epoch 520 Loss:0.0432\n",
      "Epoch 530 Loss:0.0418\n",
      "Epoch 540 Loss:0.0405\n",
      "Epoch 550 Loss:0.0393\n",
      "Epoch 560 Loss:0.0381\n",
      "Epoch 570 Loss:0.0370\n",
      "Epoch 580 Loss:0.0360\n",
      "Epoch 590 Loss:0.0350\n",
      "Epoch 600 Loss:0.0341\n",
      "Epoch 610 Loss:0.0332\n",
      "Epoch 620 Loss:0.0324\n",
      "Epoch 630 Loss:0.0316\n",
      "Epoch 640 Loss:0.0308\n",
      "Epoch 650 Loss:0.0301\n",
      "Epoch 660 Loss:0.0294\n",
      "Epoch 670 Loss:0.0287\n",
      "Epoch 680 Loss:0.0281\n",
      "Epoch 690 Loss:0.0275\n",
      "Epoch 700 Loss:0.0269\n",
      "Epoch 710 Loss:0.0263\n",
      "Epoch 720 Loss:0.0258\n",
      "Epoch 730 Loss:0.0253\n",
      "Epoch 740 Loss:0.0248\n",
      "Epoch 750 Loss:0.0243\n",
      "Epoch 760 Loss:0.0238\n",
      "Epoch 770 Loss:0.0234\n",
      "Epoch 780 Loss:0.0230\n",
      "Epoch 790 Loss:0.0225\n",
      "Epoch 800 Loss:0.0221\n",
      "Epoch 810 Loss:0.0217\n",
      "Epoch 820 Loss:0.0214\n",
      "Epoch 830 Loss:0.0210\n",
      "Epoch 840 Loss:0.0206\n",
      "Epoch 850 Loss:0.0203\n",
      "Epoch 860 Loss:0.0200\n",
      "Epoch 870 Loss:0.0197\n",
      "Epoch 880 Loss:0.0193\n",
      "Epoch 890 Loss:0.0190\n",
      "Epoch 900 Loss:0.0187\n",
      "Epoch 910 Loss:0.0185\n",
      "Epoch 920 Loss:0.0182\n",
      "Epoch 930 Loss:0.0179\n",
      "Epoch 940 Loss:0.0177\n",
      "Epoch 950 Loss:0.0174\n",
      "Epoch 960 Loss:0.0172\n",
      "Epoch 970 Loss:0.0169\n",
      "Epoch 980 Loss:0.0167\n",
      "Epoch 990 Loss:0.0165\n",
      "Epoch 1000 Loss:0.0162\n"
     ]
    }
   ],
   "source": [
    "w1,b1,w2,b2 = train(X_circle,y_circle,2,4,2,lr = 0.5,epochs = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "db79e884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      " [[0.03013057 0.96986943]\n",
      " [0.01246535 0.98753465]\n",
      " [0.00231727 0.99768273]\n",
      " ...\n",
      " [0.98324737 0.01675263]\n",
      " [0.99733314 0.00266686]\n",
      " [0.97706495 0.02293505]]\n",
      "Predicted classes: [1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1\n",
      " 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1\n",
      " 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1\n",
      " 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0\n",
      " 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
      " 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0\n",
      " 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
      " 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1\n",
      " 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
      " 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0\n",
      " 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1\n",
      " 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0\n",
      " 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0\n",
      " 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1\n",
      " 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1\n",
      " 1 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0\n",
      " 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1\n",
      " 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0\n",
      " 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0\n",
      " 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0\n",
      " 0]\n",
      "True classes: [1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1\n",
      " 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1\n",
      " 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1\n",
      " 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0\n",
      " 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
      " 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0\n",
      " 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
      " 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1\n",
      " 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
      " 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0\n",
      " 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1\n",
      " 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0\n",
      " 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0\n",
      " 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1\n",
      " 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1\n",
      " 1 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0\n",
      " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0\n",
      " 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1\n",
      " 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0\n",
      " 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0\n",
      " 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred_circle, _ = forward(X_circle,w1,b1,w2,b2)\n",
    "print(\"Probabilities:\\n\", y_pred_circle)\n",
    "print(\"Predicted classes:\", np.argmax(y_pred_circle, axis=1))\n",
    "print(\"True classes:\", np.argmax(y_circle, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8e08960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = np.argmax(y_pred_circle, axis = 1)\n",
    "true_class = np.argmax(y_circle, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "75e24014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_class == true_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b858e3",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c655d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train,y_train) , (x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1dc3ce0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_test.reshape(x_test.shape[0],-1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c06b9d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_one_hot(y_test,num_classes = 10)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "78d2676a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss:2.6503\n",
      "Epoch 20 Loss:2.3210\n",
      "Epoch 30 Loss:2.3048\n",
      "Epoch 40 Loss:2.3015\n",
      "Epoch 50 Loss:2.3010\n",
      "Epoch 60 Loss:2.3009\n",
      "Epoch 70 Loss:2.3009\n",
      "Epoch 80 Loss:2.3008\n",
      "Epoch 90 Loss:2.3008\n",
      "Epoch 100 Loss:2.3008\n",
      "Epoch 110 Loss:2.3008\n",
      "Epoch 120 Loss:2.3008\n",
      "Epoch 130 Loss:2.3008\n",
      "Epoch 140 Loss:2.3008\n",
      "Epoch 150 Loss:2.3008\n",
      "Epoch 160 Loss:2.3008\n",
      "Epoch 170 Loss:2.3008\n",
      "Epoch 180 Loss:2.3008\n",
      "Epoch 190 Loss:2.3008\n",
      "Epoch 200 Loss:2.3008\n",
      "Epoch 210 Loss:2.3008\n",
      "Epoch 220 Loss:2.3008\n",
      "Epoch 230 Loss:2.3008\n",
      "Epoch 240 Loss:2.3008\n",
      "Epoch 250 Loss:2.3008\n",
      "Epoch 260 Loss:2.3008\n",
      "Epoch 270 Loss:2.3008\n",
      "Epoch 280 Loss:2.3008\n",
      "Epoch 290 Loss:2.3008\n",
      "Epoch 300 Loss:2.3008\n",
      "Epoch 310 Loss:2.3008\n",
      "Epoch 320 Loss:2.3008\n",
      "Epoch 330 Loss:2.3008\n",
      "Epoch 340 Loss:2.3008\n",
      "Epoch 350 Loss:2.3008\n",
      "Epoch 360 Loss:2.3008\n",
      "Epoch 370 Loss:2.3008\n",
      "Epoch 380 Loss:2.3008\n",
      "Epoch 390 Loss:2.3008\n",
      "Epoch 400 Loss:2.3008\n",
      "Epoch 410 Loss:2.3008\n",
      "Epoch 420 Loss:2.3008\n",
      "Epoch 430 Loss:2.3008\n",
      "Epoch 440 Loss:2.3008\n",
      "Epoch 450 Loss:2.3008\n",
      "Epoch 460 Loss:2.3008\n",
      "Epoch 470 Loss:2.3008\n",
      "Epoch 480 Loss:2.3008\n",
      "Epoch 490 Loss:2.3008\n",
      "Epoch 500 Loss:2.3008\n",
      "Epoch 510 Loss:2.3008\n",
      "Epoch 520 Loss:2.3008\n",
      "Epoch 530 Loss:2.3008\n",
      "Epoch 540 Loss:2.3008\n",
      "Epoch 550 Loss:2.3008\n",
      "Epoch 560 Loss:2.3008\n",
      "Epoch 570 Loss:2.3008\n",
      "Epoch 580 Loss:2.3008\n",
      "Epoch 590 Loss:2.3008\n",
      "Epoch 600 Loss:2.3008\n",
      "Epoch 610 Loss:2.3008\n",
      "Epoch 620 Loss:2.3008\n",
      "Epoch 630 Loss:2.3008\n",
      "Epoch 640 Loss:2.3008\n",
      "Epoch 650 Loss:2.3008\n",
      "Epoch 660 Loss:2.3008\n",
      "Epoch 670 Loss:2.3008\n",
      "Epoch 680 Loss:2.3008\n",
      "Epoch 690 Loss:2.3008\n",
      "Epoch 700 Loss:2.3008\n",
      "Epoch 710 Loss:2.3008\n",
      "Epoch 720 Loss:2.3008\n",
      "Epoch 730 Loss:2.3008\n",
      "Epoch 740 Loss:2.3008\n",
      "Epoch 750 Loss:2.3008\n",
      "Epoch 760 Loss:2.3008\n",
      "Epoch 770 Loss:2.3008\n",
      "Epoch 780 Loss:2.3008\n",
      "Epoch 790 Loss:2.3008\n",
      "Epoch 800 Loss:2.3008\n",
      "Epoch 810 Loss:2.3008\n",
      "Epoch 820 Loss:2.3008\n",
      "Epoch 830 Loss:2.3008\n",
      "Epoch 840 Loss:2.3008\n",
      "Epoch 850 Loss:2.3008\n",
      "Epoch 860 Loss:2.3008\n",
      "Epoch 870 Loss:2.3008\n",
      "Epoch 880 Loss:2.3008\n",
      "Epoch 890 Loss:2.3008\n",
      "Epoch 900 Loss:2.3008\n",
      "Epoch 910 Loss:2.3008\n",
      "Epoch 920 Loss:2.3008\n",
      "Epoch 930 Loss:2.3008\n",
      "Epoch 940 Loss:2.3008\n",
      "Epoch 950 Loss:2.3008\n",
      "Epoch 960 Loss:2.3008\n",
      "Epoch 970 Loss:2.3008\n",
      "Epoch 980 Loss:2.3008\n",
      "Epoch 990 Loss:2.3008\n",
      "Epoch 1000 Loss:2.3008\n"
     ]
    }
   ],
   "source": [
    "w1,b1,w2,b2 = train(x,y,784,20,10,lr = 0.9,epochs = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a778fa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      " [[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Predicted classes: [1 1 1 ... 1 1 1]\n",
      "True classes: [7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "y_pred, _ = forward(x,w1,b1,w2,b2)\n",
    "print(\"Probabilities:\\n\", y)\n",
    "print(\"Predicted classes:\", np.argmax(y_pred, axis=1))\n",
    "print(\"True classes:\", np.argmax(y, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "380975de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_class == true_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b2605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
